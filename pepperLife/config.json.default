{
  "connection": {
    "_comment": "Paramètres pour la connexion au robot NAOqi.",
    "ip": "127.0.0.1",
    "port": 9559
  },
  "mqtt": {
    "_comment": "Paramètres MQTT pour synchroniser les chorégraphies multi-robots.",
    "enabled": true,
    "broker_url": "mqtt://moz4r.hd.free.fr:1883",
    "_comment_credentials": "Laissez vide pour utiliser les variables d'environnement PEPPER_MQTT_USER / PEPPER_MQTT_PASS.",
    "username": "pepper",
    "password": "pepperlife***",
    "_comment_client_id": "Identifiant client MQTT (laisser vide pour auto-génération).",
    "client_id": "",
    "_comment_keepalive": "Intervalle keepalive MQTT (secondes).",
    "keepalive": 60,
    "_comment_topics": "Préfixe commun aux topics utilisés par PepperLife.",
    "topic_prefix": "pepperlife/choreo",
    "presence_topic": "pepperlife/choreo/presence",
    "command_topic": "pepperlife/choreo/commands",
    "_comment_room": "Nom de salon ou suffixe permettant de regrouper plusieurs robots (sera complété automatiquement).",
    "room_code": "",
    "_comment_presence_interval": "Fréquence (secondes) de republication de présence.",
    "presence_interval": 30,
    "_comment_presence_qos": "QoS pour les messages de présence.",
    "presence_qos": 0,
    "_comment_insecure_tls": "true uniquement si vous utilisez un certificat auto-signé.",
    "allow_insecure_tls": false,
    "_comment_debug": "Active les logs MQTT détaillés coté client.",
    "debug": false
  },
  "stt": {
    "_comment": "Configuration de la reconnaissance vocale.",
    "engine": "openai",
    "_comment_model": "Modèle OpenAI lorsque engine=openai.",
    "model": "gpt-4o-transcribe",
    "_comment_language": "Langue cible pour la transcription.",
    "language": "fr",
    "_comment_local": "Paramètres pour un serveur Whisper local.",
    "local_server_url": "",
    "health_endpoint": "/health",
    "transcribe_endpoint": "/transcribe",
    "timeout": 15
  },
  "audio": {
    "_comment": "Configuration pour le traitement audio et la détection de la parole (VAD).",
    "_comment_vad": "vad_level (1-5) contrôle la sensibilité de la détection vocale (1=très sensible, 5=peu sensible). override_base_sensitivity, s'il est défini, remplace la calibration automatique du bruit.",
    "vad_level": 3,
    "override_base_sensitivity": null,
    "_comment_preroll": "Nb de chunks audio (10ms chacun) gardés en mémoire avant le début de la parole. Augmenter si le début des phrases est coupé.",
    "preroll_chunks": 16,
    "agc_target": 20000,
    "speech_cooldown": 2.0,
    "add_wait_tag": true
  },
  "openai": {
    "_comment": "Configuration pour les modèles OpenAI, le prompt système et la clé API. Si laissée vide, la variable d'environnement OPENAI_API_KEY sera utilisée.",
    "api_key": "",
    "stt_model": "gpt-4o-transcribe",
    "_comment_models": "Le modèle de chat à utiliser. `gpt-5` active des options spécifiques.",
    "chat_model": "gpt-4o",
    "_comment_temperature": "Contrôle le caractère aléatoire. Non utilisé par les modèles gpt-5.",
    "temperature": 0.2,
    "_comment_reasoning": "Niveau de raisonnement (pour les modèles gpt-5 uniquement). Peut être `null` ou `\"none\"` pour désactiver.",
    "reasoning_effort": "low",
    "_comment_stream": "Active le streaming pour le mode debug GPT.",
    "stream": true,
    "_comment_history": "Longueur de l'historique de chat à conserver (en nombre de messages). Ex: 4 pour 2 échanges.",
    "history_length": 4,
    "custom_prompt": "Ton nom est pepper",
    "_comment_maxtokens": "Nombre maximum de tokens (mots/ponctuation) que le modèle peut générer dans une réponse.",
    "max_output_tokens": 4096,
    "_comment_verbosity": "Contrôle le bavardage de la réponse: low, medium, high.",
    "text_verbosity": "low"
  },
  "ollama": {
    "_comment": "Configuration pour un serveur Ollama distant.",
    "_comment_servers": "Liste des serveurs connus. Les adresses peuvent être saisies au format IP:port ou URL complète.",
    "preferred_servers": [
      "http://127.0.0.1:11434"
    ],
    "_comment_active": "Serveur actuellement sélectionné pour le chat Ollama.",
    "active_server": "",
    "_comment_model": "Nom du modèle Ollama à utiliser (ex: llama3:8b).",
    "chat_model": "llama3.1:8b",
    "_comment_prompt": "Prompt personnalisé spécifique à Ollama (ajouté avant le prompt système).",
    "custom_prompt": "",
    "_comment_temperature": "Paramètre de température envoyé dans options.temperature.",
    "temperature": 0.1,
    "_comment_top_p": "Paramètre top-p (nucleus sampling) envoyé dans options.top_p.",
    "top_p": 0.9,
    "_comment_top_k": "Paramètre top-k, restreint le sampling aux k tokens les plus probables.",
    "top_k": 35,
    "_comment_min_p": "Paramètre min-p (probabilité minimale cumulée) envoyé dans options.min_p.",
    "min_p": 0.08,
    "_comment_repeat_penalty": "Pénalité de répétition appliquée via options.repeat_penalty.",
    "repeat_penalty": 1.25,
    "_comment_repeat_last_n": "Fenêtre de tokens prise en compte pour la pénalité de répétition.",
    "repeat_last_n": 512,
    "_comment_mirostat": "Active (2) ou désactive (0) le mode Mirostat.",
    "mirostat": 2,
    "_comment_mirostat_tau": "Paramètre tau pour Mirostat (contrôle la perplexité cible).",
    "mirostat_tau": 5.0,
    "_comment_mirostat_eta": "Paramètre eta pour Mirostat (taux d'ajustement).",
    "mirostat_eta": 0.1,
    "_comment_seed": "Graine de génération pour assurer la reproductibilité (mettre null pour désactiver).",
    "seed": 42,
    "_comment_stream": "Active le flux streaming d'Ollama (true conseillé).",
    "stream": true,
    "_comment_history": "Longueur de l'historique de chat retenue (en nombre de messages).",
    "history_length": 4,
    "_comment_max_tokens": "Nombre maximum de tokens générés (options.num_predict). Laisser null pour défaut serveur.",
    "max_output_tokens": 256,
    "_comment_keep_alive": "Valeur keep_alive pour Ollama (ex: \"5m\" ou \"\" pour défaut).",
    "keep_alive": "5m",
    "_comment_num_ctx": "Taille maximale du contexte (options.num_ctx).",
    "num_ctx": 2048,
    "_comment_stop": "Liste de séquences de stop à transmettre (laisser vide pour désactiver).",
    "stop": [
      "<|eot_id|>",
      "<|end_of_text|>"
    ],
    "_comment_timeout": "Timeout en secondes pour les requêtes Ollama.",
    "timeout": 15,
    "_comment_wait_tag": "Tag ^wait spécifique à Ollama.",
    "add_wait_tag": true,
    "_comment_startup_anim": "Animation de démarrage quand Ollama est actif.",
    "enable_startup_animation": true,
    "_comment_thinking": "Gestes de réflexion pour Ollama.",
    "enable_thinking_gesture": true
  },
  "vision": {
    "model": "gpt-4o",
    "system_prompt": "Tu es le module de vision du robot. Tu réponds en français. Quand une image est fournie, décris brièvement la scène (2 phrases max), puis réponds précisément à la demande de l’utilisateur (ex: compter des doigts, lire un chiffre, reconnaître un objet). Évite les spéculations si l’image est ambiguë explique la source d’incertitude.",
    "triggers": [
      "que vois-tu",
      "que vois tu",
      "décris ce que tu vois",
      "regarde",
      "analyse l'image",
      "analyse l’image",
      "combien de doigts",
      "combien de doigts je montre",
      "compte les doigts"
    ]
  },
  "asr_filters": {
    "_comment": "Configuration pour le filtrage ASR.",
    "blacklist_strict": [
      "je suis", 
      "c est", 
      "c'est", 
      "je suis.", 
      "c est.", 
      "c'est.",
      "leurs parents",
      "leurs parents."
    ]
  },
  "log": {
    "_comment": "Niveau de verbosité: 0=erreur, 1=avertissement, 2=info, 3=debug.",
    "verbosity": 2
  },
  "boot": {
    "_comment": "Configuration pour le démarrage du robot.",
    "boot_vieAutonome": true,
    "boot_reveille": true,
    "start_chatbot_on_boot": false,
    "_comment_auto_mode": "Mode de chat lancé automatiquement: none, gpt, ollama.",
    "auto_chat_mode": "none",
    "autostart_pepperlife": false
  },
  "animations": {
    "_comment": "Configuration pour activer/désactiver certaines animations.",
    "enable_startup_animation": true,
    "enable_thinking_gesture": true
  }
}
